{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-zXLoY9gfTS",
        "outputId": "7af2bc49-665c-4ab8-ec67-8066cbc92989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import re\n",
        "\n",
        "dp = \"/content/drive/MyDrive/project/\"\n",
        "df = pd.read_excel(dp + \"Input.xlsx\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/gdrive/MyDrive/project\"\n",
        "test_folder = os.path.join(base_dir, \"test\")\n",
        "\n",
        "# Create the test folder if it doesn't exist\n",
        "if not os.path.exists(test_folder):\n",
        "    os.makedirs(test_folder)\n",
        "\n",
        "# Iterate through each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    url = row.get('URL')\n",
        "    url_id = row.get('URL_ID')\n",
        "\n",
        "    if not url or not url_id:\n",
        "        print(\"Missing URL or URL_ID for index:\", index)\n",
        "        continue\n",
        "\n",
        "    # Make a request to the URL\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # raise exception for bad status codes\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(\"Error fetching URL:\", e)\n",
        "        continue\n",
        "\n",
        "    # Create a BeautifulSoup object\n",
        "    try:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    except Exception as e:\n",
        "        print(\"Error creating BeautifulSoup object:\", e)\n",
        "        continue\n",
        "\n",
        "    # Find title\n",
        "    title = soup.find('h1')\n",
        "    if not title:\n",
        "        print(\"Title not found for URL_ID:\", url_id)\n",
        "        continue\n",
        "    title_text = title.get_text()\n",
        "\n",
        "    # Find text\n",
        "    article = \"\"\n",
        "    for p in soup.find_all('p'):\n",
        "        article += p.get_text()\n",
        "\n",
        "    # Write title and text to the file\n",
        "    file_name = os.path.join(test_folder, '{}.txt'.format(url_id))\n",
        "    try:\n",
        "        with open(file_name, 'w') as file:\n",
        "            file.write(title_text + '\\n' + article)\n",
        "        print(\"File saved:\", file_name)  # Add logging for successful file save\n",
        "    except Exception as e:\n",
        "        print(\"Error writing to file:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0jK4DEPl8Sg",
        "outputId": "e19bce06-40b0-4a05-ee96-1732fcb428ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved: /gdrive/MyDrive/project/test/blackassign0001.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0002.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0003.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0004.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0005.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0006.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0007.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0008.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0009.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0010.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0011.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0012.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0013.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0014.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0015.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0016.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0017.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0018.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0019.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0020.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0021.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0022.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0023.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0024.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0025.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0026.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0027.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0028.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0029.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0030.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0031.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0032.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0033.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0034.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0035.txt\n",
            "Error fetching URL: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0037.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0038.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0039.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0040.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0041.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0042.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0043.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0044.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0045.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0046.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0047.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0048.txt\n",
            "Error fetching URL: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0050.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0051.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0052.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0053.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0054.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0055.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0056.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0057.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0058.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0059.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0060.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0061.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0062.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0063.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0064.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0065.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0066.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0067.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0068.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0069.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0070.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0071.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0072.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0073.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0074.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0075.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0076.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0077.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0078.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0079.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0080.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0081.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0082.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0083.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0084.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0085.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0086.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0087.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0088.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0089.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0090.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0091.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0092.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0093.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0094.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0095.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0096.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0097.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0098.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0099.txt\n",
            "File saved: /gdrive/MyDrive/project/test/blackassign0100.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directories\n",
        "base_dir = \"/content/drive/MyDrive/project/stopwords/\"\n",
        "text_dir = \"/content/drive/MyDrive/project/test/\"\n",
        "stopwords_dir = \"/content/drive/MyDrive/project/stopwords/combined_text.txt\"\n",
        "\n",
        "# Common encodings to try\n",
        "encodings = ['utf-8', 'latin-1', 'ISO-8859-1']\n",
        "\n",
        "# List to store content of each text file\n",
        "all_text = []\n",
        "\n",
        "# Load text files from base directory\n",
        "for file_name in os.listdir(base_dir):\n",
        "    if file_name.endswith('.txt'):\n",
        "        file_path = os.path.join(base_dir, file_name)\n",
        "        text = None\n",
        "        # Attempt to open the file with different encodings\n",
        "        for encoding in encodings:\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding=encoding) as file:\n",
        "                    text = file.read()\n",
        "                break  # Stop trying other encodings if successful\n",
        "            except UnicodeDecodeError:\n",
        "                print(f\"Failed to decode {file_path} with encoding {encoding}\")\n",
        "        if text is not None:\n",
        "            all_text.append(text)\n",
        "\n",
        "# Combine text from all files into a single string\n",
        "combined_text = '\\n'.join(all_text)\n",
        "\n",
        "# Save combined text to a new file\n",
        "combined_file_path = os.path.join(base_dir, \"combined_text.txt\")\n",
        "with open(combined_file_path, 'w') as combined_file:\n",
        "    combined_file.write(combined_text)\n",
        "\n",
        "# Load stop words from the stopwords directory and store in the set variable\n",
        "stop_words = set()\n",
        "with open(stopwords_dir, 'r', encoding='ISO-8859-1') as f:\n",
        "    stop_words.update(set(f.read().splitlines()))\n",
        "\n",
        "# Load all text files from the text directory and store in a list (docs)\n",
        "docs = []\n",
        "for text_file in os.listdir(text_dir):\n",
        "    with open(os.path.join(text_dir, text_file), 'r') as f:\n",
        "        text = f.read()\n",
        "        # Tokenize the text file\n",
        "        words = word_tokenize(text)\n",
        "        # Remove stop words from the tokens\n",
        "        filtered_text = [word for word in words if word.lower() not in stop_words]\n",
        "        # Add filtered tokens of each file into a list\n",
        "        docs.append(filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6CKpBSEl9Y4",
        "outputId": "074a6107-d162-46c3-f763-90c328dc0cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to decode /content/drive/MyDrive/project/stopwords/StopWords_Currencies.txt with encoding utf-8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store positive and negative words from the directory\n",
        "sentment_dir = \"/content/drive/MyDrive/project/MasterDictionary/\"\n",
        "pos = set()\n",
        "neg = set()\n",
        "\n",
        "for filename in os.listdir(sentment_dir):\n",
        "    with open(os.path.join(sentment_dir, filename), 'r', encoding='ISO-8859-1') as f:\n",
        "        words = f.read().splitlines()\n",
        "        if filename == 'positive-words.txt':\n",
        "            pos.update(words)\n",
        "        else:\n",
        "            neg.update(words)\n",
        "\n",
        "# Collect positive and negative words from each file and calculate scores\n",
        "positive_words = []\n",
        "negative_words = []\n",
        "positive_score = []\n",
        "negative_score = []\n",
        "polarity_score = []\n",
        "subjectivity_score = []\n",
        "\n",
        "# Iterate through the list of docs\n",
        "for doc in docs:\n",
        "    positive_words.append([word for word in doc if word.lower() in pos])\n",
        "    negative_words.append([word for word in doc if word.lower() in neg])\n",
        "    positive_score.append(len(positive_words[-1]))\n",
        "    negative_score.append(len(negative_words[-1]))\n",
        "    denominator = len(doc) + 0.000001  # Small value added to avoid division by zero\n",
        "    polarity_score.append((positive_score[-1] - negative_score[-1]) / denominator)\n",
        "    subjectivity_score.append((positive_score[-1] + negative_score[-1]) / denominator)\n",
        "\n",
        "stopwords = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "0wC_wzjSmCOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def measure(file):\n",
        "    with open(os.path.join(text_dir, file), 'r') as f:\n",
        "        text = f.read()\n",
        "        text = re.sub(r'[^\\w\\s.]', '', text)\n",
        "        sentences = text.split('.')\n",
        "        num_sentences = len(sentences)\n",
        "        words = [word for word in text.split() if word.lower() not in stopwords]\n",
        "        num_words = len(words)\n",
        "\n",
        "        complex_words = [word for word in words if len([letter for letter in word if letter.lower() in 'aeiou']) > 2]\n",
        "\n",
        "        syllable_count = 0\n",
        "        syllable_words = []\n",
        "        for word in words:\n",
        "            if word.endswith('es'):\n",
        "                word = word[:-2]\n",
        "            elif word.endswith('ed'):\n",
        "                word = word[:-2]\n",
        "            syllable_count_word = sum(1 for letter in word if letter.lower() in 'aeiou')\n",
        "            if syllable_count_word >= 1:\n",
        "                syllable_words.append(word)\n",
        "                syllable_count += syllable_count_word\n",
        "\n",
        "        avg_sentence_len = num_words / num_sentences\n",
        "        avg_syllable_word_count = syllable_count / len(syllable_words)\n",
        "        percent_complex_words = len(complex_words) / num_words\n",
        "        fog_index = 0.4 * (avg_sentence_len + percent_complex_words)\n",
        "\n",
        "        return avg_sentence_len, percent_complex_words, fog_index, len(complex_words), avg_syllable_word_count\n",
        "\n",
        "avg_sentence_length = []\n",
        "percentage_of_complex_words = []\n",
        "fog_index = []\n",
        "complex_word_count = []\n",
        "avg_syllable_word_count = []\n",
        "\n",
        "for file in os.listdir(text_dir):\n",
        "    x, y, z, a, b = measure(file)\n",
        "    avg_sentence_length.append(x)\n",
        "    percentage_of_complex_words.append(y)\n",
        "    fog_index.append(z)\n",
        "    complex_word_count.append(a)\n",
        "    avg_syllable_word_count.append(b)\n",
        "\n",
        "def cleaned_words(file):\n",
        "    with open(os.path.join(text_dir, file), 'r') as f:\n",
        "        text = f.read()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        words = [word for word in text.split() if word.lower() not in stopwords]\n",
        "        length = sum(len(word) for word in words)\n",
        "        average_word_length = length / len(words)\n",
        "    return len(words), average_word_length\n",
        "\n",
        "word_count = []\n",
        "average_word_length = []\n",
        "\n",
        "for file in os.listdir(text_dir):\n",
        "    x, y = cleaned_words(file)\n",
        "    word_count.append(x)\n",
        "    average_word_length.append(y)\n",
        "\n",
        "def count_personal_pronouns(file):\n",
        "    with open(os.path.join(text_dir, file), 'r') as f:\n",
        "        text = f.read()\n",
        "        personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
        "        count = 0\n",
        "        for pronoun in personal_pronouns:\n",
        "            count += len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text))\n",
        "    return count\n",
        "\n",
        "pp_count = []\n",
        "\n",
        "for file in os.listdir(text_dir):\n",
        "    x = count_personal_pronouns(file)\n",
        "    pp_count.append(x)\n",
        "\n",
        "output_df = pd.read_excel(dp + 'Output Data Structure.xlsx')\n",
        "indices_to_drop = [44-37, 57-37, 144-37]\n",
        "\n",
        "# Check if the indices exist in the DataFrame before dropping\n",
        "indices_to_drop = [idx for idx in indices_to_drop if idx in output_df.index]\n",
        "\n",
        "# Drop the rows from the DataFrame\n",
        "if indices_to_drop:\n",
        "    output_df.drop(indices_to_drop, axis=0, inplace=True)\n",
        "\n",
        "variables = [positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length,\n",
        "             percentage_of_complex_words, fog_index, avg_sentence_length, complex_word_count, word_count,\n",
        "             avg_syllable_word_count, pp_count, average_word_length]\n",
        "\n",
        "for i, var in enumerate(variables):\n",
        "    column_index = i + 2  # Adjust column index based on your DataFrame structure\n",
        "    if len(var) != len(output_df):\n",
        "        # Handle the length mismatch silently\n",
        "        pass\n",
        "    else:\n",
        "        output_df.iloc[:, column_index] = var\n",
        "\n",
        "output_df.to_csv('Output_Data.csv')"
      ],
      "metadata": {
        "id": "L6CEtARHmKuP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}